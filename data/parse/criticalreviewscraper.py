# -*- coding: utf-8 -*-
"""shibbolethAuth.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KgERnM1OsJd3UVYI1bg5Iwq3zIV7zKsl

## Scraping Critical Review 
_________________

I use bs4, mechanize, and requests to scrape Critical Review. With the data from courses.js, the list of courses on Critical Review, I scrape each course and insert into the courses database, coursesDb.sqlite3.


**Steps to Scraping:** 
- Get list representation of all courses
- Pass Shibboleth and DUO Authentication
- Scrape each individual link 
- Add to database for each coures

Using !pip command to install necessary libraries, then importing.
"""

# Please install the following libraries in terminal, and run this code independently!
# pip install bs4
# pip install mechanize
# pip install requests
# pip install -U -q PyDrive

# from google.colab import drive
# from bs4 import BeautifulSoup as bs
# import requests
# import mechanize
# import urllib
# import sys
# import re
# import sqlite3
#
# """### Establishing Connection to Google Drive"""
#
# drive.mount('/content/drive/',force_remount=True)
# path = '/content/drive/Shared drives/CS32-Final/Data/coursesDB.db'
#
# conn = sqlite3.connect(path)
# c = conn.cursor()
# # When using SQLite, include the following line to ensure foreign key commands are recognized
# c.execute('PRAGMA foreign_keys = ON')
#
# #We want to access courses.js
# !ls "/content/drive/Shared drives/CS32-Final/Data/"
#
# """### Adding Courses to the CourseList
#
# Reading the courses.js file to add all the course information available on Critical Review to the database. Each course is formatted:
#
# > "course: MCM 0110: Introduction to the Theory and Analysis of Modern Culture and Media"
#
# so we have to parse each to get the necessary information -> "MCM 0110"
# """
#
# with open("/content/drive/Shared drives/CS32-Final/Data/courses.js") as f:
#   lines = f.readlines()
# neededLines = lines[8::9]
# courseList = []
# for each in neededLines:
#   each = each.strip()
#   each = each[11:-1]
#
#   #splits to get the department name
#   res = each.split(" ")
#   courseID = res[1]
#
#   courseList.append(res[0] + " " + courseID[:-1])
#
# """Prints out courseList for debugging and viewing purposes.
#
# We find that there are 3001 courses with reviews on Critical Review.
# """
#
# count = 0
# for course in courseList:
#   count += 1
#   if count < 10:
#     print(course + " " + str(count))
#
# """### Scraping Example: One Page
#
# The following code scrapes one page as an example. First, we're given a string, "course".
#
# From this, we have to parse the department and classID.
#
# ```
# course = "MCM 0110"
# department = "MCM"
# classID = "0110"
# ```
#
# After parsing the correct URL for critical review searching, we establish a connection the the page.
# """
#
# # Sets up URL
# course = "CSCI 0150"
# res = course.split(" ")
# department = res[0]
# classID = res[1]
#
# url = 'https://thecriticalreview.org/search/' + department + '/' + classID
# print(url)
#
# response = requests.get(url)
# soup = bs(response.text, "html.parser")
#
# #Prints content at url:
# # print(soup)
#
# """We can find that the authentication information is held inside a table with the form and login information.
#
# Printing this out, we see that we must pass SAML and SSO redirection.
# """
#
# authInfo = soup.find('form', id='login')
#
# print(authInfo)
#
# """Then, I use Mechanize to browse and fill in the login form, adding my own cookies to the headers.
#
# After passing the DUO and Shibboleth Authentication, we can parse the page we want to.
# """
#
# br = mechanize.Browser()
# br.set_handle_robots(False)
# br.set_handle_equiv(True)
# br.set_handle_gzip(True)
# br.set_handle_redirect(True)
# br.set_handle_refresh(False)
# br.set_handle_referer(True)
# br.set_handle_robots(False)
#
# br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)
#
# br.addheaders = [('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'), ('Cookie', 'Cookie: _ga=GA1.2.477472401.1583779863; connect.sid=s%3AI0SrIAsB-MOwNuvkCDRijo1294S-mv86.dFOkFUU1ETqDzEkZiOSzCNEzKHtHDzLZzb1tU7crV0U; _gid=GA1.2.893140995.1586274034; _gat=1')]
#
# response = br.open(url)
# response = br.response().read()
# soup = bs(response, "html.parser")
#
# """Data is held inside of a table, "ui tiny statistic" on the right of every critical review page.
#
# We can parse each table by its values, getting the top 6 values. Then we pick the ones that we need.
# """
#
# tables = soup.findAll("div", {"class": "ui tiny statistic"})
#
# parse = soup.find_all('div', attrs={'class':'value'})
# res = []
# count = 0
# for i in parse:
#   if (count < 6):
#     res.append(i.text.strip())
#     count+=1
#
# #print results
# if (len(res) == 0):
#   courseRate = "n/a"
#   profRate = "n/a"
#   avgHour = "n/a"
#   maxHour = "n/a"
#   classSize = "n/a"
# else:
#   courseRate = res[0]
#   profRate = res[1]
#   avgHour = res[2]
#   maxHour = res[3]
#   classSize = res[5]
#
# print(courseRate)
# print(profRate)
# print(avgHour)
# print(maxHour)
# print(classSize)
#
# """We have been able to successfully scrape one page! Now we can put this code in a for loop so that we can process all the courses in courseList.
#
# Then we can add each course's information to the database.
#
#
# __________
#
# ### Scraping each course in the Database
# """
#
# count = 0
# for course in courseList:
#   print("-------------------------")
#
#   res = course.split(" ")
#   department = res[0]
#   classID = res[1]
#   print(department +  " " + classID)
#
#   url = 'https://thecriticalreview.org/search/' + department + '/' + classID
#
#   br = mechanize.Browser()
#   br.addheaders = [('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36'), ('Cookie', 'Cookie: _ga=GA1.2.477472401.1583779863; connect.sid=s%3AI0SrIAsB-MOwNuvkCDRijo1294S-mv86.dFOkFUU1ETqDzEkZiOSzCNEzKHtHDzLZzb1tU7crV0U; _gid=GA1.2.893140995.1586274034; _gat=1')]
#
#   response = br.open(url)
#   response = br.response().read()
#   soup = bs(response, "html.parser")
#
#   tables = soup.findAll("div", {"class": "ui tiny statistic"})
#
#
#   parse = soup.find_all('div', attrs={'class':'value'})
#   res = []
#   count = 0
#   for i in parse:
#     if (count < 6):
#       res.append(i.text.strip())
#       count+=1
#     else:
#       break
#
#   if (len(res) == 0):
#     courseRate = "n/a"
#     avgHour = "n/a"
#     maxHour = "n/a"
#     classSize = "n/a"
#   else:
#     courseRate = res[0]
#     avgHour = res[2]
#     maxHour = res[3]
#     classSize = res[5]
#
#   print(courseRate)
#   print(avgHour)
#   print(maxHour)
#   print(classSize)
#   print(url)
#
#   className = department + " " + classID
#   #insert into db here
#   row = ["CourseID", "courseName", "prereqs1, prereqs2", "semestersOffered", "professorName", "courseRating", "avg_hrs", "max_hrs", "CR_link", "class_size"]
#   c.execute('INSERT INTO courses VALUES (?,?,?,?,?,?,?,?,?,?);''', (className,
#             "n/a", "n/a","n/a", "n/a", res[0], res[2], res[3], url, res[5]))
#   conn.commit()

